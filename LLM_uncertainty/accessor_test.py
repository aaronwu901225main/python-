from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
from datasets import load_dataset, Dataset, load_from_disk
import torch
import re
from datetime import datetime
import json
import os

TEST_SIZE = 'all'

MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"

REPEAT = 1
CONTEXT_LENGTH = 32000
MAX_TOKENS = 30000
TEMPERATURE = 0.6
TOP_P = 0.95

start_time = datetime.now()
filename = start_time.strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs("./report", exist_ok=True)

llm = LLM(
    model=MODEL_ID,
    dtype=torch.float16,
    trust_remote_code=True,
    quantization='bitsandbytes',
    load_format='bitsandbytes',
    max_model_len=CONTEXT_LENGTH,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

root_dir = "./mmlu_splits"
# subjects = [name for name in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, name))]
subjects = [name for name in os.listdir(root_dir) 
            if os.path.isdir(os.path.join(root_dir, name)) and name.startswith("college")]
print(subjects)

for subject in subjects:
    print(f"\nğŸ§ª Evaluating subject: {subject}")
    data = load_from_disk(os.path.join(root_dir, subject, "train"))
    
    if TEST_SIZE != 'all':
        data = data.select(range(TEST_SIZE))
    
    def extract_boxed_answer(text: str) -> str:
        matches = re.findall(r'\\boxed\{(.*?)\}', text)
        return matches[-1].strip() if matches else None
    
    params = SamplingParams(
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        top_p=TOP_P,
    )
    
    results = {"correct": 0, "wrong": 0}
    choices = ['A', 'B', 'C', 'D']
    correct_cots = []
    wrong_cots = []
    
    for i, d in enumerate(data):
        user_prompt = (
            f"Solve the following multiple choices question:\n{d['question']}\n"
            f"A) {d['choices'][0]}\n"
            f"B) {d['choices'][1]}\n"
            f"C) {d['choices'][2]}\n"
            f"D) {d['choices'][3]}\n"
            "Please reason step by step, and put your final answer within \\boxed{}.For example if the answer is A ,then output \\boxed{A}"
        )
    
        prompt =f"<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>{user_prompt}\n<ï½œAssistantï½œ><think>"

        output1 = llm.generate(prompt, params)
        
        response_1 = output1[0].outputs[0].text +'\nFinal answer: \\boxed{'

        output2 = llm.generate(prompt + response_1, params)

        response_2 = output2[0].outputs[0].text
        
        response = response_1 + response_2

        extracted_answer = extract_boxed_answer(response)
    
        print(prompt + response)
        print(f'\n{subject} Question {i + 1}/{len(data)}')
        print(f'res toks 1: {len(output1[0].outputs[0].token_ids)}')
        print(f'res toks 2: {len(output2[0].outputs[0].token_ids)}')
        print(f'Extracted answer: {extracted_answer}')
        print(f'True answer: {choices[d["answer"]]}')
    
        cot = {
            'question': user_prompt,
            'CoT': '<think>' + response,
            'extracted_answer': extracted_answer,
            'true_answer': choices[d["answer"]],
            'label': 1 if extracted_answer == choices[d["answer"]] else 0,
        }
    
        if extracted_answer == choices[d["answer"]]:
            results["correct"] += 1
            correct_cots.append(cot)
        else:
            results["wrong"] += 1
            wrong_cots.append(cot)
    
        print(f"Current results: {results}")
        print('Spend time:', datetime.now() - start_time)
        print('=' * 20)
        
    # å„²å­˜å„ç§‘ç›®çµæœï¼ˆé€™æ˜¯æ–°å¢çš„ï¼‰
    with open(f"./report/correct_{subject}_{filename}.json", "w") as f:
        json.dump(correct_cots, f, indent=4)

    with open(f"./report/wrong_{subject}_{filename}.json", "w") as f:
        json.dump(wrong_cots, f, indent=4)

    # é‡è¨­è³‡æ–™ä»¥é€²è¡Œä¸‹ä¸€å€‹ subject çš„è™•ç†
    correct_cots = []
    wrong_cots = []
    results = {"correct": 0, "wrong": 0}
