{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原本的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "NEW_MODEL=\"meta-llama/Llama-3.1-8B-Instruct\"  # replace the output-dir/checkpoint-xx\n",
    "\n",
    "# load trained/resized tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)\n",
    "\n",
    "# here we are loading the raw model, if you can't load it on your GPU, you can just change device_map to cpu\n",
    "# we won't need gpu here anyway\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "\n",
    "system = \"You are a Artificial Intelligence assistant and willing to answer the question from the user.\"  \n",
    "user = \"What we might do after building a XCAT phantom\"   #  replace your question / prompt\n",
    "# use chat template\n",
    "text = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|>  \n",
    "<|start_header_id|>user<|end_header_id|>{user}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n'''   \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=4096,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune 的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEW_MODEL=\"llama3-fake2-5-r32/checkpoint-3600\"  # replace the output-dir/checkpoint-xx\n",
    "\n",
    "# load trained/resized tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)\n",
    "\n",
    "# here we are loading the raw model, if you can't load it on your GPU, you can just change device_map to cpu\n",
    "# we won't need gpu here anyway\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "model = PeftModel.from_pretrained(model, NEW_MODEL)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "system = \"You are a Artificial Intelligence assistant and willing to answer the question from the user.\"  \n",
    "user = \"What we might do after building a XCAT phantom\"   #  replace your question / prompt\n",
    "# use chat template\n",
    "text = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|>  \n",
    "<|start_header_id|>user<|end_header_id|>{user}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n'''   \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=4096,)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
