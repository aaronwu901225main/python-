{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2025 DL Lab5: Object Detection on Pascal VOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, please put **your name** and **SID** in following format: <br>\n",
    "Hi I'm 陸仁賈, 314831000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm 吳禎哲, 313833003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project focuses on object detection using the Pascal VOC dataset. \n",
    "\n",
    "The goal is to identify and locate various objects within images by training and evaluating detection models.\n",
    " \n",
    "The dataset provides annotated images across multiple categories, making it a standard benchmark for evaluating object detection performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition\n",
    "Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish datasets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n",
    "\n",
    "This assignment use kaggle to calculate your grade.  \n",
    "Please use this [**LINK**](https://www.kaggle.com/t/e86ea95cb007416a85a07d8729ac838e) to join the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Data\n",
    "\n",
    "Unzip `dataset.zip` \n",
    "\n",
    "+ `vocall_test.txt` : list for the training set\n",
    "+ `vocall_test.txt` : list for the validation set\n",
    "+ `vocall_test.txt` : list for the test set\n",
    "+ `image/` : contains all images.\n",
    "\n",
    "\n",
    "The train set contains 8,218 images, the val set contains 3,823 images, and the test set contains 8,920 images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You are allowed to use a **backbone model**, but only those available from the **timm package** (https://huggingface.co/timm/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.amp import autocast, GradScaler\n",
    "from src.yolo import getODmodel\n",
    "from yolo_loss import YOLOv3Loss\n",
    "from src.dataset import VocDetectorDataset, train_data_pipelines, test_data_pipelines, collate_fn\n",
    "from src.eval_voc import evaluate\n",
    "from src.config import GRID_SIZES, ANCHORS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import weave\n",
    "import wandb\n",
    "os.environ['WANDB_API_KEY'] = '38feb6c50e85ec500d8a256415926ee90e4b5094'\n",
    "weave.init('LAB5-YOLOv3-Object-Detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####hyperparameters#####\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "lambda_coord=5.0\n",
    "lambda_obj=1.0\n",
    "lambda_noobj=0.5\n",
    "lambda_class=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Weights & Biases 追蹤\n",
    "project_name = 'LAB5-YOLOv3-Object-Detection'\n",
    "run = wandb.init(project=project_name, config={\n",
    "    'device': str(device),\n",
    "    'num_epochs': num_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'lambda_coord': lambda_coord,\n",
    "    'lambda_obj': lambda_obj,\n",
    "    'lambda_noobj': lambda_noobj,\n",
    "    'lambda_class': lambda_class,\n",
    "})\n",
    "# 定義指標與步進軸\n",
    "wandb.define_metric('epoch')\n",
    "wandb.define_metric('train/*', step_metric='epoch')\n",
    "wandb.define_metric('val/*', step_metric='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "file_root_train = './dataset/image/'\n",
    "annotation_file_train = './dataset/vocall_train.txt'\n",
    "file_root_val = './dataset/image/'\n",
    "annotation_file_val = './dataset/vocall_val.txt'\n",
    " # Data paths\n",
    "file_root_train = './dataset/image/'\n",
    "annotation_file_train = './dataset/vocall_train.txt'\n",
    "file_root_val = './dataset/image/'\n",
    "annotation_file_val = './dataset/vocall_val.txt'\n",
    "\n",
    "# Create datasets\n",
    "print('Loading datasets...')\n",
    "train_dataset = VocDetectorDataset(\n",
    "    root_img_dir=file_root_train,\n",
    "    dataset_file=annotation_file_train,\n",
    "    train=True,\n",
    "    transform=train_data_pipelines,\n",
    "    grid_sizes=GRID_SIZES,\n",
    "    encode_target=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "print(f'Loaded {len(train_dataset)} train images')\n",
    "\n",
    "val_dataset = VocDetectorDataset(\n",
    "    root_img_dir=file_root_val,\n",
    "    dataset_file=annotation_file_val,\n",
    "    train=False,\n",
    "    transform=test_data_pipelines,\n",
    "    grid_sizes=GRID_SIZES,\n",
    "    encode_target=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "#for computing val maps\n",
    "eval_dataset = VocDetectorDataset(\n",
    "    root_img_dir=file_root_val,\n",
    "    dataset_file=annotation_file_val,\n",
    "    train=False,\n",
    "    transform=test_data_pipelines,\n",
    "    grid_sizes=GRID_SIZES,\n",
    "    encode_target=False,\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "print(f'Loaded {len(val_dataset)} val images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only backbone model on timm is acceptable (https://huggingface.co/timm/models).\n",
    "### You can modify model name in yolo class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_network_path = None #'checkpoints/best_detector.pth' \n",
    "pretrained = True\n",
    "model = getODmodel(pretrained=pretrained).to(device)\n",
    "\n",
    "# 記錄模型梯度與權重\n",
    "wandb.watch(model, log='gradients', log_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some training utils, use mix precision if valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "criterion = YOLOv3Loss(lambda_coord, lambda_obj, lambda_noobj, lambda_class, ANCHORS).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = GradScaler(enabled=use_amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立紀錄容器：每個 epoch 紀錄一次\n",
    "import numpy as np\n",
    "history = {\n",
    "    'train_total': [],\n",
    "    'val_total': [],\n",
    "    'map': [],\n",
    "    'train_box': [],\n",
    "    'train_obj': [],\n",
    "    'train_noobj': [],\n",
    "    'train_cls': [],\n",
    "    'val_box': [],\n",
    "    'val_obj': [],\n",
    "    'val_noobj': [],\n",
    "    'val_cls': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print('\\nStarting training...')\n",
    "torch.cuda.empty_cache()\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f'\\n\\nStarting epoch {epoch + 1} / {num_epochs}')\n",
    "\n",
    "    # 累積訓練 loss（本 epoch 平均）\n",
    "    train_sum = {k: 0.0 for k in ['total','box','obj','noobj','cls']}\n",
    "    train_batches = 0\n",
    "\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        target = [t.to(device) for t in target]\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(\"cuda\", enabled=use_amp):\n",
    "            pred = model(images)\n",
    "            # pred and target are lists of each scales\n",
    "            loss_dict = criterion(pred, target)\n",
    "        # Backward pass with mixed precision support\n",
    "        scaler.scale(loss_dict['total']).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # 累積訓練統計\n",
    "        for k in train_sum:\n",
    "            train_sum[k] += float(loss_dict[k].detach().cpu())\n",
    "        train_batches += 1\n",
    "\n",
    "        # Print progress\n",
    "        if i % 50 == 0:\n",
    "            outstring = f'Epoch [{epoch+1}/{num_epochs}], Iter [{i+1}/{len(train_loader)}], Loss: '\n",
    "            outstring += ', '.join(f\"{key}={val :.3f}\" for key, val in loss_dict.items())\n",
    "            print(outstring)\n",
    "\n",
    "    # epoch-end：學習率與平均訓練損失\n",
    "    lr_scheduler.step()\n",
    "    learning_rate = lr_scheduler.get_last_lr()[0]\n",
    "    print(f'Learning Rate for this epoch: {learning_rate}')\n",
    "\n",
    "    train_avg = {k: (train_sum[k] / max(train_batches, 1)) for k in train_sum}\n",
    "    history['train_total'].append(train_avg['total'])\n",
    "    history['train_box'].append(train_avg['box'])\n",
    "    history['train_obj'].append(train_avg['obj'])\n",
    "    history['train_noobj'].append(train_avg['noobj'])\n",
    "    history['train_cls'].append(train_avg['cls'])\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_sum = {k: 0.0 for k in ['total','box','obj','noobj','cls']}\n",
    "        model.eval()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            target = [t.to(device) for t in target]\n",
    "            # Forward pass\n",
    "            pred = model(images)\n",
    "            loss_dict = criterion(pred, target)\n",
    "            for k in val_sum:\n",
    "                val_sum[k] += float(loss_dict[k].detach().cpu())\n",
    "\n",
    "        val_avg = {k: (val_sum[k] / max(len(val_loader), 1)) for k in val_sum}\n",
    "        print(f'Validation Loss: {val_avg[\"total\"]:.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if best_val_loss > val_avg['total']:\n",
    "        best_val_loss = val_avg['total']\n",
    "        print(f'Updating best val loss: {best_val_loss:.5f}')\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        torch.save(model.state_dict(), 'checkpoints/best_detector.pth')\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) in [5, 10, 20, 30, 40]:\n",
    "        torch.save(model.state_dict(), f'checkpoints/detector_epoch_{epoch+1}.pth')\n",
    "\n",
    "    torch.save(model.state_dict(), 'checkpoints/detector.pth')\n",
    "\n",
    "    # 紀錄驗證平均損失\n",
    "    history['val_total'].append(val_avg['total'])\n",
    "    history['val_box'].append(val_avg['box'])\n",
    "    history['val_obj'].append(val_avg['obj'])\n",
    "    history['val_noobj'].append(val_avg['noobj'])\n",
    "    history['val_cls'].append(val_avg['cls'])\n",
    "\n",
    "    # Wandb: 紀錄 epoch 統計\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'lr': learning_rate,\n",
    "        'train/total': train_avg['total'],\n",
    "        'train/box': train_avg['box'],\n",
    "        'train/obj': train_avg['obj'],\n",
    "        'train/noobj': train_avg['noobj'],\n",
    "        'train/cls': train_avg['cls'],\n",
    "        'val/total': val_avg['total'],\n",
    "        'val/box': val_avg['box'],\n",
    "        'val/obj': val_avg['obj'],\n",
    "        'val/noobj': val_avg['noobj'],\n",
    "        'val/cls': val_avg['cls'],\n",
    "    }, step=epoch+1)\n",
    "\n",
    "    # Evaluate on val set（每 5 個 epoch 計算一次 mAP）\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('\\nEvaluating on validation set...')\n",
    "        val_aps = evaluate(model, eval_loader)\n",
    "        cur_map = float(np.mean(val_aps)) if len(val_aps) else 0.0\n",
    "        print(f'Epoch {epoch+1}, mAP: {cur_map:.4f}')\n",
    "        history['map'].append(cur_map)\n",
    "        wandb.log({'epoch': epoch + 1, 'val/mAP': cur_map}, step=epoch+1)\n",
    "    else:\n",
    "        # 用 NaN 佔位，方便等長繪圖\n",
    "        history['map'].append(float('nan'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化：Loss 與 ACC(mAP)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, len(history['train_total']) + 1))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, history['train_total'], label='train')\n",
    "plt.plot(epochs, history['val_total'], label='val')\n",
    "plt.title('Loss (total)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, history['map'], label='mAP', color='tab:green')\n",
    "plt.title('ACC (mAP)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('mAP')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation loss curves and mAP (ACC)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(1, len(history['train_total']) + 1)\n",
    "\n",
    "# === Figure 1: Total Loss + mAP ===\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history['train_total'], label='train_total')\n",
    "plt.plot(epochs, history['val_total'], label='val_total')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# mAP may be None/NaN on non-eval epochs; mask them\n",
    "map_vals = np.array([np.nan if v is None else v for v in history['map']], dtype=float)\n",
    "plt.plot(epochs, map_vals, marker='o', label='mAP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('mAP')\n",
    "plt.title('Validation mAP (ACC)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig_total_map = plt.gcf()\n",
    "plt.show()\n",
    "\n",
    "# === Figure 2: component-wise loss trends (facet 2x2) ===\n",
    "plt.figure(figsize=(12, 8))\n",
    "comp_keys = [('box','Box'), ('obj','Obj'), ('noobj','NoObj'), ('cls','Cls')]\n",
    "for idx, (k, name) in enumerate(comp_keys, start=1):\n",
    "    plt.subplot(2, 2, idx)\n",
    "    plt.plot(epochs, history[f'train_{k}'], label=f'train_{name.lower()}')\n",
    "    plt.plot(epochs, history[f'val_{k}'], label=f'val_{name.lower()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{name} Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "fig_components = plt.gcf()\n",
    "plt.show()\n",
    "\n",
    "# === Log figures to Weights & Biases ===\n",
    "try:\n",
    "    wandb.log({'fig/loss_total_map': fig_total_map, 'fig/loss_components': fig_components})\n",
    "except Exception as e:\n",
    "    print('wandb.log(fig) skipped:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結束 W&B run（可選）\n",
    "try:\n",
    "    wandb.finish()\n",
    "except Exception as e:\n",
    "    print('wandb.finish() skipped:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Result\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/e86ea95cb007416a85a07d8729ac838e).\n",
    "\n",
    "**How to upload**\n",
    "\n",
    "1. Click the folder icon in the left hand side of Colab.\n",
    "2. Right click \"result.csv\". Select \"Download\"\n",
    "3. To kaggle. Click \"Submit Predictions\"\n",
    "4. Upload the result.csv\n",
    "5. System will automaticlaly calculate the accuracy of 50% dataset and publish this result to leaderboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict_test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
